# -微博社区划分和热点词分析
本项目分为4个部分：
爬虫 
数据处理和社区划分 
文本训练的模型 
前端页面

1. 我们爬取了大概79万条微博信息，噪声信息十分多，比如抽奖微博，一些营销账号的微博，以及正常微博内容中的各类特殊字符。因此需要对这些微博进行过滤。
（因为官方的反爬虫措施，爬虫部分目前可能已经失效）


2.正则清洗结果分析
用户在发布微博的时候，会产生和附带一些无用的信息，这些信息对话题的发现没有实际的帮助，因此需要进行过滤处理。
我们使用正则表达式对微博数据进行了初步的清洗，减小了原始数据的干扰性。
微博内容中常见的网页链接，地址分享，@用户名等内容被有效的清洗。


3.停用词(Stop Word)是在处理文本数据之前或者之后过滤掉的一些无用的字或者词，从而达到节约空间、降低噪声的目的。
在微博数据中适当地减少Stop Words出现的频率，可以有效地帮助我们提高关键词密度，而在关键词列表中避免出现Stop Words往往能够让我们优化的关键词更突出，
在我们的项目中使用的是百度停用词表和一些微博内容常见词汇。


